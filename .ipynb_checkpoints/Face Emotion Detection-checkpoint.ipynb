{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "986c5a25-bd25-45ae-b8dc-d23ca53b12b5",
   "metadata": {},
   "source": [
    "# Image Processing and Analysis\n",
    "\n",
    "- Optical Mark Recognition\n",
    "- Face Emotion Detection\n",
    "- Color Detection\n",
    "- Background Removal\n",
    "- Fruit Classifier\n",
    "- Face Mask Detection\n",
    "- Cookie Cutter Squid Game\n",
    "- Green Light Red Light Squid Game\n",
    "- Hand Distance Measurement\n",
    "- Real-time Digit Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8b38b8-d4ff-4859-9dcc-1b783a6a2848",
   "metadata": {},
   "source": [
    "# Face Emotion Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a98008-0ca2-4d38-93d4-994726275931",
   "metadata": {},
   "source": [
    "## Modul yang diperlukan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3fae82bf-444b-4586-b2c4-e0dc9463882e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "from keras_preprocessing.image import load_img\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Dropout, Flatten, MaxPooling2D\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a584fc98-b410-411d-b736-3dd46e1508f8",
   "metadata": {},
   "source": [
    "## Persiapan Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64ca43c7-0470-4188-8202-4ab1d66c40f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DIR = 'images/train'\n",
    "TEST_DIR = 'images/validation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21bc7f58-a70b-4d5d-a591-52c6d54a1474",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createdataframe(dir):\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "    for label in os.listdir(dir):\n",
    "        for imagename in os.listdir(os.path.join(dir, label)):\n",
    "            image_paths.append(os.path.join(dir, label, imagename))\n",
    "            labels.append(label)\n",
    "        print(label, \"completed\")\n",
    "    return image_paths, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2156248f-9058-4c5e-abaa-5200b81ac56f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "happy completed\n",
      "sad completed\n",
      "fear completed\n",
      "surprise completed\n",
      "neutral completed\n",
      "angry completed\n",
      "disgust completed\n",
      "                                image    label\n",
      "0         images/train/happy/3578.jpg    happy\n",
      "1        images/train/happy/16988.jpg    happy\n",
      "2         images/train/happy/2666.jpg    happy\n",
      "3         images/train/happy/5109.jpg    happy\n",
      "4        images/train/happy/11981.jpg    happy\n",
      "...                               ...      ...\n",
      "28816  images/train/disgust/10112.jpg  disgust\n",
      "28817  images/train/disgust/21668.jpg  disgust\n",
      "28818   images/train/disgust/7049.jpg  disgust\n",
      "28819   images/train/disgust/9716.jpg  disgust\n",
      "28820   images/train/disgust/3561.jpg  disgust\n",
      "\n",
      "[28821 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "train = pd.DataFrame()\n",
    "train['image'], train['label'] = createdataframe(TRAIN_DIR)\n",
    "print(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b6c8cfa4-2dae-4ce7-ab6b-562e170dd486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "happy completed\n",
      "sad completed\n",
      "fear completed\n",
      "surprise completed\n",
      "neutral completed\n",
      "angry completed\n",
      "disgust completed\n",
      "                                    image    label\n",
      "0       images/validation/happy/23933.jpg    happy\n",
      "1       images/validation/happy/24906.jpg    happy\n",
      "2       images/validation/happy/18033.jpg    happy\n",
      "3       images/validation/happy/15271.jpg    happy\n",
      "4       images/validation/happy/26888.jpg    happy\n",
      "...                                   ...      ...\n",
      "7061  images/validation/disgust/20761.jpg  disgust\n",
      "7062  images/validation/disgust/28710.jpg  disgust\n",
      "7063  images/validation/disgust/23876.jpg  disgust\n",
      "7064   images/validation/disgust/9460.jpg  disgust\n",
      "7065  images/validation/disgust/35580.jpg  disgust\n",
      "\n",
      "[7066 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "test = pd.DataFrame()\n",
    "test['image'], test['label'] = createdataframe(TEST_DIR)\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a982d1c-d9b5-412c-8363-ae84d645e9bd",
   "metadata": {},
   "source": [
    " ## Pemrosesan Awal Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9fd14d7c-41fe-4a50-a5b8-eb081e67f54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f52fdda4-be28-40e2-82c6-426b21279093",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(images):\n",
    "    features = []\n",
    "    for image in tqdm(images):\n",
    "        img = load_img(image, grayscale=True)\n",
    "        img = np.array(img)\n",
    "        features.append(img)\n",
    "    features = np.array(features)\n",
    "    features = features.reshape(len(features), 48, 48, 1)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c756755b-6c51-47ba-a535-fd8cefdc2efb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2a356b59b034aecb4f41ec231baabbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28821 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/keras_preprocessing/image/utils.py:107: UserWarning: grayscale is deprecated. Please use color_mode = \"grayscale\"\n",
      "  warnings.warn('grayscale is deprecated. Please use '\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "570288fd98884e0e94e619ace6708d87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7066 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_features = extract_features(train['image'])\n",
    "test_features = extract_features(test['image'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c26da8-5a60-4575-a151-7263a424807c",
   "metadata": {},
   "source": [
    "## Normalisasi:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5c7c9c89-8b3d-4e03-9e5f-4431776f33e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_features / 255.0\n",
    "x_test = test_features / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "08057cb7-cb89-410c-9323-5ca7d992cd9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "le.fit(train['label'])\n",
    "y_train = le.transform(train['label'])\n",
    "y_test = le.transform(test['label'])\n",
    "y_train = to_categorical(y_train, num_classes=7)\n",
    "y_test = to_categorical(y_test, num_classes=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b250ec1-7fea-4a14-973b-c5e49d8a11c5",
   "metadata": {},
   "source": [
    "## Membangun Model Jaringan Neural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ba29f826-f7f6-462c-af9e-5eb5be9f7784",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# convolutional layers\n",
    "model.add(Conv2D(128, kernel_size=(3,3), activation='relu', input_shape=(48,48,1)))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Conv2D(256, kernel_size=(3,3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Conv2D(512, kernel_size=(3,3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Conv2D(512, kernel_size=(3,3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "# flattening\n",
    "model.add(Flatten())\n",
    "\n",
    "# fully connected layers\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "# output layer\n",
    "model.add(Dense(7, activation='softmax'))\n",
    "\n",
    "# model compilation\n",
    "model.compile(optimizer = 'adam', loss = 'categorical_crossentropy',metrics = ['accuracy'] )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883f8f6b-10d2-449e-9aa4-804a98fa4a8c",
   "metadata": {},
   "source": [
    "##  Melatih Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2613e3d-2ff0-495c-bb2f-641e416a3098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m262s\u001b[0m 1s/step - accuracy: 0.2252 - loss: 1.8466 - val_accuracy: 0.2583 - val_loss: 1.8125\n",
      "Epoch 2/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m283s\u001b[0m 1s/step - accuracy: 0.2493 - loss: 1.8138 - val_accuracy: 0.2634 - val_loss: 1.7920\n",
      "Epoch 3/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m281s\u001b[0m 1s/step - accuracy: 0.2640 - loss: 1.7685 - val_accuracy: 0.3262 - val_loss: 1.6595\n",
      "Epoch 4/100\n",
      "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m280s\u001b[0m 1s/step - accuracy: 0.3356 - loss: 1.6578 - val_accuracy: 0.4329 - val_loss: 1.4725\n",
      "Epoch 5/100\n",
      "\u001b[1m 91/226\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m2:43\u001b[0m 1s/step - accuracy: 0.3923 - loss: 1.5393"
     ]
    }
   ],
   "source": [
    "model.fit(x=x_train, y=y_train, batch_size=128, epochs=100, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dca93b8-3ebb-4c61-bb2a-66442483e119",
   "metadata": {},
   "source": [
    "## Model Menyimpan Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b336e51c-c8cd-49ce-91df-feb0136e08eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_json = model.to_json()\n",
    "with open(\"emotiondetector.json\", 'w') as json_file:\n",
    "    json_file.write(model_json)\n",
    "model.save(\"emotiondetector.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
